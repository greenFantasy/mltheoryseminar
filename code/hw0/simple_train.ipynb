{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.56.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.6 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.0a0+d31eafa)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (6.1.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.6 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install: tqdm (progress bars)\n",
    "!pip install tqdm\n",
    "!pip install torchvision\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torchvision.datasets as ds\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data (CIFAR-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def load_cifar(datadir='./data_cache'): # will download ~400MB of data into this dir. Change the dir if neccesary. If using paperspace, you can make this /storage\n",
    "    train_ds = ds.CIFAR10(root=datadir, train=True,\n",
    "                           download=True, transform=None)\n",
    "    test_ds = ds.CIFAR10(root=datadir, train=False,\n",
    "                          download=True, transform=None)\n",
    "\n",
    "    def to_xy(dataset):\n",
    "        X = torch.Tensor(np.transpose(dataset.data, (0, 3, 1, 2))).float() / 255.0  # [0, 1]\n",
    "        Y = torch.Tensor(np.array(dataset.targets)).long()\n",
    "        return X, Y\n",
    "\n",
    "    X_tr, Y_tr = to_xy(train_ds)\n",
    "    X_te, Y_te = to_xy(test_ds)\n",
    "    return X_tr, Y_tr, X_te, Y_te\n",
    "\n",
    "def make_loader(dataset, batch_size=128):\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "            shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "X_tr, Y_tr, X_te, Y_te = load_cifar()\n",
    "train_dl = make_loader(TensorDataset(X_tr, Y_tr))\n",
    "test_dl = make_loader(TensorDataset(X_te, Y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dl : DataLoader, opt, k = 50, printer = True):\n",
    "    ''' Trains model for one epoch on the provided dataloader, with optimizer opt. Logs stats every k batches.'''\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    model.cuda()\n",
    "\n",
    "    netLoss = 0.0\n",
    "    nCorrect = 0\n",
    "    nTotal = 0\n",
    "    iterator = tqdm(train_dl) if printer else train_dl\n",
    "    for i, (xB, yB) in enumerate(iterator):\n",
    "        opt.zero_grad()\n",
    "        xB, yB = xB.cuda(), yB.cuda()\n",
    "        outputs = model(xB)\n",
    "        loss = loss_func(outputs, yB)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        netLoss += loss.item() * len(xB)\n",
    "        with torch.no_grad():\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            nCorrect += (preds == yB).float().sum()\n",
    "            nTotal += preds.size(0)\n",
    "        \n",
    "        if (i+1) % k == 0:\n",
    "            train_acc = nCorrect/nTotal\n",
    "            avg_loss = netLoss/nTotal\n",
    "            print(f'\\t [Batch {i+1} / {len(train_dl)}] Train Loss: {avg_loss:.3f} \\t Train Acc: {train_acc:.3f}')\n",
    "  \n",
    "    train_acc = nCorrect/nTotal\n",
    "    avg_loss = netLoss/nTotal\n",
    "    return avg_loss, train_acc\n",
    "\n",
    "\n",
    "def evaluate(model, test_dl, loss_func=nn.CrossEntropyLoss().cuda()):\n",
    "    ''' Returns loss, acc'''\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    nCorrect = 0.0\n",
    "    nTotal = 0\n",
    "    net_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (xb, yb) in test_dl:\n",
    "            xb, yb = xb.cuda(), yb.cuda()\n",
    "            outputs = model(xb)\n",
    "            loss = len(xb) * loss_func(outputs, yb)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            nCorrect += (preds == yb).float().sum()\n",
    "            net_loss += loss\n",
    "            nTotal += preds.size(0)\n",
    "\n",
    "    acc = nCorrect.cpu().item() / float(nTotal)\n",
    "    loss = net_loss.cpu().item() / float(nTotal)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5-Layer CNN for CIFAR\n",
    "## This is the Myrtle5 network by David Page (https://myrtle.ai/learn/how-to-train-your-resnet-4-architecture/)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), x.size(1))\n",
    "\n",
    "def make_cnn(c=64, num_classes=10):\n",
    "    ''' Returns a 5-layer CNN with width parameter c. '''\n",
    "    return nn.Sequential(\n",
    "        # Layer 0\n",
    "        nn.Conv2d(3, c, kernel_size=3, stride=1,\n",
    "                  padding=1, bias=True),\n",
    "        nn.BatchNorm2d(c),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        # Layer 1\n",
    "        nn.Conv2d(c, c*2, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True),\n",
    "        nn.BatchNorm2d(c*2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "\n",
    "        # Layer 2\n",
    "        nn.Conv2d(c*2, c*4, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True),\n",
    "        nn.BatchNorm2d(c*4),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "\n",
    "        # Layer 3\n",
    "        nn.Conv2d(c*4, c*8, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True),\n",
    "        nn.BatchNorm2d(c*8),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "\n",
    "        # Layer 4\n",
    "        nn.MaxPool2d(4),\n",
    "        Flatten(),\n",
    "        nn.Linear(c*8, num_classes, bias=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee850164eed44d6abe42b2f823f5264a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 5.476 \t Train Acc: 0.166\n",
      "\t [Batch 100 / 391] Train Loss: 3.826 \t Train Acc: 0.190\n",
      "\t [Batch 150 / 391] Train Loss: 3.230 \t Train Acc: 0.213\n",
      "\t [Batch 200 / 391] Train Loss: 2.906 \t Train Acc: 0.231\n",
      "\t [Batch 250 / 391] Train Loss: 2.691 \t Train Acc: 0.250\n",
      "\t [Batch 300 / 391] Train Loss: 2.539 \t Train Acc: 0.265\n",
      "\t [Batch 350 / 391] Train Loss: 2.418 \t Train Acc: 0.281\n",
      "Epoch 0:\t Train Loss: 2.338 \t Train Acc: 0.293\t Test Acc: 0.351\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4617456372a4f93b70eb8844c501d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 1.564 \t Train Acc: 0.423\n",
      "\t [Batch 100 / 391] Train Loss: 1.533 \t Train Acc: 0.439\n",
      "\t [Batch 150 / 391] Train Loss: 1.514 \t Train Acc: 0.448\n",
      "\t [Batch 200 / 391] Train Loss: 1.491 \t Train Acc: 0.457\n",
      "\t [Batch 250 / 391] Train Loss: 1.467 \t Train Acc: 0.466\n",
      "\t [Batch 300 / 391] Train Loss: 1.438 \t Train Acc: 0.477\n",
      "\t [Batch 350 / 391] Train Loss: 1.410 \t Train Acc: 0.488\n",
      "Epoch 1:\t Train Loss: 1.391 \t Train Acc: 0.495\t Test Acc: 0.495\n",
      "Starting Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e4a511eabf47de8eae86cc9007ef28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 1.171 \t Train Acc: 0.578\n",
      "\t [Batch 100 / 391] Train Loss: 1.164 \t Train Acc: 0.580\n",
      "\t [Batch 150 / 391] Train Loss: 1.143 \t Train Acc: 0.588\n",
      "\t [Batch 200 / 391] Train Loss: 1.137 \t Train Acc: 0.592\n",
      "\t [Batch 250 / 391] Train Loss: 1.128 \t Train Acc: 0.595\n",
      "\t [Batch 300 / 391] Train Loss: 1.110 \t Train Acc: 0.602\n",
      "\t [Batch 350 / 391] Train Loss: 1.103 \t Train Acc: 0.606\n",
      "Epoch 2:\t Train Loss: 1.089 \t Train Acc: 0.611\t Test Acc: 0.513\n",
      "Starting Epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698c9fb3a4ec432a808d581654597764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 0.930 \t Train Acc: 0.679\n",
      "\t [Batch 100 / 391] Train Loss: 0.928 \t Train Acc: 0.677\n",
      "\t [Batch 150 / 391] Train Loss: 0.921 \t Train Acc: 0.677\n",
      "\t [Batch 200 / 391] Train Loss: 0.908 \t Train Acc: 0.681\n",
      "\t [Batch 250 / 391] Train Loss: 0.903 \t Train Acc: 0.683\n",
      "\t [Batch 300 / 391] Train Loss: 0.897 \t Train Acc: 0.684\n",
      "\t [Batch 350 / 391] Train Loss: 0.893 \t Train Acc: 0.686\n",
      "Epoch 3:\t Train Loss: 0.888 \t Train Acc: 0.689\t Test Acc: 0.644\n",
      "Starting Epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f82a0a48cf49cf8703cde3d581714c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 0.781 \t Train Acc: 0.729\n",
      "\t [Batch 100 / 391] Train Loss: 0.768 \t Train Acc: 0.734\n",
      "\t [Batch 150 / 391] Train Loss: 0.774 \t Train Acc: 0.731\n",
      "\t [Batch 200 / 391] Train Loss: 0.773 \t Train Acc: 0.730\n",
      "\t [Batch 250 / 391] Train Loss: 0.778 \t Train Acc: 0.729\n",
      "\t [Batch 300 / 391] Train Loss: 0.772 \t Train Acc: 0.731\n",
      "\t [Batch 350 / 391] Train Loss: 0.767 \t Train Acc: 0.732\n",
      "Epoch 4:\t Train Loss: 0.766 \t Train Acc: 0.733\t Test Acc: 0.726\n",
      "Starting Epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bf8480394c480ea405e8e2456b636e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 0.696 \t Train Acc: 0.748\n",
      "\t [Batch 100 / 391] Train Loss: 0.680 \t Train Acc: 0.757\n",
      "\t [Batch 150 / 391] Train Loss: 0.680 \t Train Acc: 0.761\n",
      "\t [Batch 200 / 391] Train Loss: 0.669 \t Train Acc: 0.766\n",
      "\t [Batch 250 / 391] Train Loss: 0.665 \t Train Acc: 0.768\n",
      "\t [Batch 300 / 391] Train Loss: 0.662 \t Train Acc: 0.770\n",
      "\t [Batch 350 / 391] Train Loss: 0.661 \t Train Acc: 0.770\n",
      "Epoch 5:\t Train Loss: 0.658 \t Train Acc: 0.770\t Test Acc: 0.731\n",
      "Starting Epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3308d78b372e4d4081688e643a4d1101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 0.581 \t Train Acc: 0.798\n",
      "\t [Batch 100 / 391] Train Loss: 0.572 \t Train Acc: 0.803\n",
      "\t [Batch 150 / 391] Train Loss: 0.574 \t Train Acc: 0.801\n",
      "\t [Batch 200 / 391] Train Loss: 0.590 \t Train Acc: 0.798\n",
      "\t [Batch 250 / 391] Train Loss: 0.584 \t Train Acc: 0.799\n",
      "\t [Batch 300 / 391] Train Loss: 0.587 \t Train Acc: 0.798\n",
      "\t [Batch 350 / 391] Train Loss: 0.586 \t Train Acc: 0.799\n",
      "Epoch 6:\t Train Loss: 0.586 \t Train Acc: 0.799\t Test Acc: 0.713\n",
      "Starting Epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc00a7b807f4bb98dcd8c16d438e946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 0.473 \t Train Acc: 0.835\n",
      "\t [Batch 100 / 391] Train Loss: 0.469 \t Train Acc: 0.836\n",
      "\t [Batch 150 / 391] Train Loss: 0.484 \t Train Acc: 0.831\n",
      "\t [Batch 200 / 391] Train Loss: 0.487 \t Train Acc: 0.831\n",
      "\t [Batch 250 / 391] Train Loss: 0.488 \t Train Acc: 0.830\n",
      "\t [Batch 300 / 391] Train Loss: 0.489 \t Train Acc: 0.830\n",
      "\t [Batch 350 / 391] Train Loss: 0.494 \t Train Acc: 0.828\n",
      "Epoch 7:\t Train Loss: 0.494 \t Train Acc: 0.829\t Test Acc: 0.753\n",
      "Starting Epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa9a40b04e147b2ba03b46478cf33cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 0.434 \t Train Acc: 0.853\n",
      "\t [Batch 100 / 391] Train Loss: 0.431 \t Train Acc: 0.854\n",
      "\t [Batch 150 / 391] Train Loss: 0.418 \t Train Acc: 0.858\n",
      "\t [Batch 200 / 391] Train Loss: 0.431 \t Train Acc: 0.853\n",
      "\t [Batch 250 / 391] Train Loss: 0.437 \t Train Acc: 0.851\n",
      "\t [Batch 300 / 391] Train Loss: 0.435 \t Train Acc: 0.852\n",
      "\t [Batch 350 / 391] Train Loss: 0.439 \t Train Acc: 0.850\n",
      "Epoch 8:\t Train Loss: 0.437 \t Train Acc: 0.851\t Test Acc: 0.710\n",
      "Starting Epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1d25ec75ce49949a50f015c2f5e1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 0.355 \t Train Acc: 0.884\n",
      "\t [Batch 100 / 391] Train Loss: 0.367 \t Train Acc: 0.881\n",
      "\t [Batch 150 / 391] Train Loss: 0.375 \t Train Acc: 0.876\n",
      "\t [Batch 200 / 391] Train Loss: 0.372 \t Train Acc: 0.876\n",
      "\t [Batch 250 / 391] Train Loss: 0.380 \t Train Acc: 0.874\n",
      "\t [Batch 300 / 391] Train Loss: 0.376 \t Train Acc: 0.874\n",
      "\t [Batch 350 / 391] Train Loss: 0.383 \t Train Acc: 0.873\n",
      "Epoch 9:\t Train Loss: 0.383 \t Train Acc: 0.872\t Test Acc: 0.711\n",
      "Starting Epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6102b14467e449bbeea78f88b5c6ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 0.289 \t Train Acc: 0.899\n",
      "\t [Batch 100 / 391] Train Loss: 0.286 \t Train Acc: 0.901\n",
      "\t [Batch 150 / 391] Train Loss: 0.306 \t Train Acc: 0.897\n",
      "\t [Batch 200 / 391] Train Loss: 0.301 \t Train Acc: 0.898\n",
      "\t [Batch 250 / 391] Train Loss: 0.306 \t Train Acc: 0.896\n",
      "\t [Batch 300 / 391] Train Loss: 0.306 \t Train Acc: 0.895\n",
      "\t [Batch 350 / 391] Train Loss: 0.313 \t Train Acc: 0.893\n",
      "Epoch 10:\t Train Loss: 0.314 \t Train Acc: 0.893\t Test Acc: 0.712\n",
      "Starting Epoch 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9eb1c145f143389b8ec5cfe6c53389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 0.213 \t Train Acc: 0.927\n",
      "\t [Batch 100 / 391] Train Loss: 0.232 \t Train Acc: 0.923\n",
      "\t [Batch 150 / 391] Train Loss: 0.255 \t Train Acc: 0.918\n",
      "\t [Batch 200 / 391] Train Loss: 0.258 \t Train Acc: 0.917\n",
      "\t [Batch 250 / 391] Train Loss: 0.264 \t Train Acc: 0.914\n",
      "\t [Batch 300 / 391] Train Loss: 0.274 \t Train Acc: 0.912\n",
      "\t [Batch 350 / 391] Train Loss: 0.275 \t Train Acc: 0.911\n",
      "Epoch 11:\t Train Loss: 0.278 \t Train Acc: 0.908\t Test Acc: 0.755\n",
      "Starting Epoch 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0ee2af38d1473a9523c2e4c35c26fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 0.141 \t Train Acc: 0.954\n",
      "\t [Batch 100 / 391] Train Loss: 0.163 \t Train Acc: 0.951\n",
      "\t [Batch 150 / 391] Train Loss: 0.160 \t Train Acc: 0.951\n",
      "\t [Batch 200 / 391] Train Loss: 0.163 \t Train Acc: 0.949\n",
      "\t [Batch 250 / 391] Train Loss: 0.177 \t Train Acc: 0.943\n",
      "\t [Batch 300 / 391] Train Loss: 0.178 \t Train Acc: 0.942\n",
      "\t [Batch 350 / 391] Train Loss: 0.193 \t Train Acc: 0.939\n",
      "Epoch 12:\t Train Loss: 0.197 \t Train Acc: 0.937\t Test Acc: 0.803\n",
      "Starting Epoch 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff2dc61e0a64851b20d1f074c691111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 0.103 \t Train Acc: 0.970\n",
      "\t [Batch 100 / 391] Train Loss: 0.101 \t Train Acc: 0.970\n",
      "\t [Batch 150 / 391] Train Loss: 0.122 \t Train Acc: 0.966\n",
      "\t [Batch 200 / 391] Train Loss: 0.130 \t Train Acc: 0.962\n",
      "\t [Batch 250 / 391] Train Loss: 0.143 \t Train Acc: 0.958\n",
      "\t [Batch 300 / 391] Train Loss: 0.144 \t Train Acc: 0.956\n",
      "\t [Batch 350 / 391] Train Loss: 0.156 \t Train Acc: 0.953\n",
      "Epoch 13:\t Train Loss: 0.164 \t Train Acc: 0.951\t Test Acc: 0.814\n",
      "Starting Epoch 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c93996b2c4478ea4599fd4166fe047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 0.059 \t Train Acc: 0.983\n",
      "\t [Batch 100 / 391] Train Loss: 0.052 \t Train Acc: 0.985\n",
      "\t [Batch 150 / 391] Train Loss: 0.052 \t Train Acc: 0.986\n",
      "\t [Batch 200 / 391] Train Loss: 0.051 \t Train Acc: 0.986\n",
      "\t [Batch 250 / 391] Train Loss: 0.103 \t Train Acc: 0.975\n",
      "\t [Batch 300 / 391] Train Loss: 0.099 \t Train Acc: 0.975\n",
      "\t [Batch 350 / 391] Train Loss: 0.098 \t Train Acc: 0.974\n",
      "Epoch 14:\t Train Loss: 0.098 \t Train Acc: 0.973\t Test Acc: 0.813\n",
      "Starting Epoch 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa7ca50b69e478499476f041ad42c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 0.031 \t Train Acc: 0.994\n",
      "\t [Batch 100 / 391] Train Loss: 0.028 \t Train Acc: 0.995\n",
      "\t [Batch 150 / 391] Train Loss: 0.026 \t Train Acc: 0.995\n",
      "\t [Batch 200 / 391] Train Loss: 0.026 \t Train Acc: 0.995\n",
      "\t [Batch 250 / 391] Train Loss: 0.027 \t Train Acc: 0.995\n",
      "\t [Batch 300 / 391] Train Loss: 0.026 \t Train Acc: 0.995\n",
      "\t [Batch 350 / 391] Train Loss: 0.026 \t Train Acc: 0.995\n",
      "Epoch 15:\t Train Loss: 0.026 \t Train Acc: 0.995\t Test Acc: 0.850\n",
      "Starting Epoch 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b2fb1cf3c447389eeb4a5f6f3b81bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 0.009 \t Train Acc: 1.000\n",
      "\t [Batch 100 / 391] Train Loss: 0.009 \t Train Acc: 1.000\n",
      "\t [Batch 150 / 391] Train Loss: 0.009 \t Train Acc: 1.000\n",
      "\t [Batch 200 / 391] Train Loss: 0.008 \t Train Acc: 1.000\n",
      "\t [Batch 250 / 391] Train Loss: 0.008 \t Train Acc: 1.000\n",
      "\t [Batch 300 / 391] Train Loss: 0.008 \t Train Acc: 1.000\n",
      "\t [Batch 350 / 391] Train Loss: 0.008 \t Train Acc: 1.000\n",
      "Epoch 16:\t Train Loss: 0.008 \t Train Acc: 1.000\t Test Acc: 0.854\n",
      "Starting Epoch 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4fd727a26c41dd9897536bdf8489c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 0.005 \t Train Acc: 1.000\n",
      "\t [Batch 100 / 391] Train Loss: 0.004 \t Train Acc: 1.000\n",
      "\t [Batch 150 / 391] Train Loss: 0.004 \t Train Acc: 1.000\n",
      "\t [Batch 200 / 391] Train Loss: 0.004 \t Train Acc: 1.000\n",
      "\t [Batch 250 / 391] Train Loss: 0.004 \t Train Acc: 1.000\n",
      "\t [Batch 300 / 391] Train Loss: 0.004 \t Train Acc: 1.000\n",
      "\t [Batch 350 / 391] Train Loss: 0.004 \t Train Acc: 1.000\n",
      "Epoch 17:\t Train Loss: 0.004 \t Train Acc: 1.000\t Test Acc: 0.859\n",
      "Starting Epoch 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359aa0fac84a496babb7ae1be2c73659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 100 / 391] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 150 / 391] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 200 / 391] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 250 / 391] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 300 / 391] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 350 / 391] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "Epoch 18:\t Train Loss: 0.003 \t Train Acc: 1.000\t Test Acc: 0.860\n",
      "Starting Epoch 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2b1e4517f6474eb652abbad4dd9f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 391] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 100 / 391] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 150 / 391] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 200 / 391] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 250 / 391] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 300 / 391] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 350 / 391] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "Epoch 19:\t Train Loss: 0.002 \t Train Acc: 1.000\t Test Acc: 0.859\n"
     ]
    }
   ],
   "source": [
    "model = make_cnn()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "epochs = 20\n",
    "for i in range(epochs):\n",
    "    print(f'Starting Epoch {i}')\n",
    "    train_loss, train_acc = train_epoch(model, train_dl, opt)\n",
    "    test_loss, test_acc = evaluate(model, test_dl)\n",
    "    \n",
    "    print(f'Epoch {i}:\\t Train Loss: {train_loss:.3f} \\t Train Acc: {train_acc:.3f}\\t Test Acc: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipping an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 12\n",
    "img = torch.flip(train_dl.dataset.tensors[0][idx], [1])\n",
    "plt.imshow(np.transpose(np.array(img), (1, 2, 0)))\n",
    "print(np.array(train_dl.dataset.tensors[1][idx]))\n",
    "\n",
    "def plot_image(img):\n",
    "    plt.imshow(np.transpose(np.array(img), (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flipping images in test set\n",
    "X_te_flipped = torch.flip(X_te, [2])\n",
    "\n",
    "test_dl_flipped = make_loader(TensorDataset(X_te_flipped, Y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9)\n"
     ]
    }
   ],
   "source": [
    "idx=1231\n",
    "plot_image(X_te[idx])\n",
    "plot_image(X_te_flipped[idx])\n",
    "print(Y_te[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_dl_flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.04576484375 0.4293\n"
     ]
    }
   ],
   "source": [
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 \t Test Accuracy: 0.882 \t Flipped Test Accuracy 0.758\t Accuracy Drop: 0.124\n",
      "Class 1 \t Test Accuracy: 0.931 \t Flipped Test Accuracy 0.381\t Accuracy Drop: 0.550\n"
     ]
    }
   ],
   "source": [
    "# Organize the test data by class\n",
    "classes = {}\n",
    "for i in range(10):\n",
    "    classes[i] = []\n",
    "\n",
    "for idx, y in enumerate(Y_te):\n",
    "    classes[int(y)].append(idx)\n",
    "\n",
    "# print('')\n",
    "    \n",
    "for label in classes:\n",
    "    idxs = np.array(classes[label])\n",
    "    X_temp = X_te[idxs]\n",
    "    X_temp_flipped = X_te_flipped[idxs]\n",
    "    Y_temp = Y_te[idxs]\n",
    "    temp_dl = make_loader(TensorDataset(X_temp, Y_temp))\n",
    "    temp_dl_flipped = make_loader(TensorDataset(X_temp_flipped, Y_temp))\n",
    "    _, test_acc = evaluate(model, temp_dl)\n",
    "    _, test_acc_flipped = evaluate(model, temp_dl_flipped)\n",
    "    print(f'Class {label} \\t Test Accuracy: {test_acc:.3f} \\t Flipped Test Accuracy {test_acc_flipped:.3f}' + \n",
    "          f'\\t Accuracy Drop: {test_acc - test_acc_flipped:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a more robust model\n",
    "\n",
    "Ideas: \n",
    " - Train with flipped and rotated images\n",
    " - More powerful adversarial training? Although this might not help much since we are not testing on these kinds of adversaries \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_flipped = torch.flip(X_tr, [2])\n",
    "\n",
    "X_tr_combined = torch.cat([X_tr, X_tr_flipped])\n",
    "\n",
    "Y_tr_combined = torch.cat([Y_tr, Y_tr])\n",
    "\n",
    "train_combined_dl = make_loader(TensorDataset(X_tr_combined, Y_tr_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1da3f4db3ee4e66a957f6370a59548d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 4.908 \t Train Acc: 0.164\n",
      "\t [Batch 100 / 782] Train Loss: 3.499 \t Train Acc: 0.188\n",
      "\t [Batch 150 / 782] Train Loss: 3.014 \t Train Acc: 0.205\n",
      "\t [Batch 200 / 782] Train Loss: 2.747 \t Train Acc: 0.223\n",
      "\t [Batch 250 / 782] Train Loss: 2.576 \t Train Acc: 0.239\n",
      "\t [Batch 300 / 782] Train Loss: 2.458 \t Train Acc: 0.252\n",
      "\t [Batch 350 / 782] Train Loss: 2.363 \t Train Acc: 0.264\n",
      "\t [Batch 400 / 782] Train Loss: 2.282 \t Train Acc: 0.275\n",
      "\t [Batch 450 / 782] Train Loss: 2.214 \t Train Acc: 0.287\n",
      "\t [Batch 500 / 782] Train Loss: 2.156 \t Train Acc: 0.298\n",
      "\t [Batch 550 / 782] Train Loss: 2.106 \t Train Acc: 0.308\n",
      "\t [Batch 600 / 782] Train Loss: 2.059 \t Train Acc: 0.320\n",
      "\t [Batch 650 / 782] Train Loss: 2.018 \t Train Acc: 0.330\n",
      "\t [Batch 700 / 782] Train Loss: 1.977 \t Train Acc: 0.340\n",
      "\t [Batch 750 / 782] Train Loss: 1.943 \t Train Acc: 0.349\n",
      "Epoch 0:\t Train Loss: 1.921 \t Train Acc: 0.355\t Test Acc: 0.312\t Test Acc Flipped: 0.308\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b045e56db524f5b8701c3f17bd97f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 1.380 \t Train Acc: 0.496\n",
      "\t [Batch 100 / 782] Train Loss: 1.365 \t Train Acc: 0.504\n",
      "\t [Batch 150 / 782] Train Loss: 1.349 \t Train Acc: 0.510\n",
      "\t [Batch 200 / 782] Train Loss: 1.339 \t Train Acc: 0.515\n",
      "\t [Batch 250 / 782] Train Loss: 1.333 \t Train Acc: 0.518\n",
      "\t [Batch 300 / 782] Train Loss: 1.322 \t Train Acc: 0.522\n",
      "\t [Batch 350 / 782] Train Loss: 1.315 \t Train Acc: 0.525\n",
      "\t [Batch 400 / 782] Train Loss: 1.302 \t Train Acc: 0.531\n",
      "\t [Batch 450 / 782] Train Loss: 1.288 \t Train Acc: 0.536\n",
      "\t [Batch 500 / 782] Train Loss: 1.275 \t Train Acc: 0.541\n",
      "\t [Batch 550 / 782] Train Loss: 1.266 \t Train Acc: 0.546\n",
      "\t [Batch 600 / 782] Train Loss: 1.255 \t Train Acc: 0.550\n",
      "\t [Batch 650 / 782] Train Loss: 1.248 \t Train Acc: 0.553\n",
      "\t [Batch 700 / 782] Train Loss: 1.240 \t Train Acc: 0.556\n",
      "\t [Batch 750 / 782] Train Loss: 1.230 \t Train Acc: 0.559\n",
      "Epoch 1:\t Train Loss: 1.224 \t Train Acc: 0.562\t Test Acc: 0.583\t Test Acc Flipped: 0.576\n",
      "Starting Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0478923091ca482fb87cd224fb491ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 1.031 \t Train Acc: 0.633\n",
      "\t [Batch 100 / 782] Train Loss: 1.010 \t Train Acc: 0.638\n",
      "\t [Batch 150 / 782] Train Loss: 1.015 \t Train Acc: 0.637\n",
      "\t [Batch 200 / 782] Train Loss: 1.019 \t Train Acc: 0.634\n",
      "\t [Batch 250 / 782] Train Loss: 1.013 \t Train Acc: 0.637\n",
      "\t [Batch 300 / 782] Train Loss: 1.010 \t Train Acc: 0.638\n",
      "\t [Batch 350 / 782] Train Loss: 1.006 \t Train Acc: 0.640\n",
      "\t [Batch 400 / 782] Train Loss: 0.999 \t Train Acc: 0.644\n",
      "\t [Batch 450 / 782] Train Loss: 0.995 \t Train Acc: 0.645\n",
      "\t [Batch 500 / 782] Train Loss: 0.992 \t Train Acc: 0.646\n",
      "\t [Batch 550 / 782] Train Loss: 0.987 \t Train Acc: 0.649\n",
      "\t [Batch 600 / 782] Train Loss: 0.980 \t Train Acc: 0.651\n",
      "\t [Batch 650 / 782] Train Loss: 0.974 \t Train Acc: 0.653\n",
      "\t [Batch 700 / 782] Train Loss: 0.971 \t Train Acc: 0.654\n",
      "\t [Batch 750 / 782] Train Loss: 0.967 \t Train Acc: 0.656\n",
      "Epoch 2:\t Train Loss: 0.965 \t Train Acc: 0.657\t Test Acc: 0.250\t Test Acc Flipped: 0.250\n",
      "Starting Epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ab9615f92f41998d37fe4f3cd182bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 0.855 \t Train Acc: 0.697\n",
      "\t [Batch 100 / 782] Train Loss: 0.863 \t Train Acc: 0.697\n",
      "\t [Batch 150 / 782] Train Loss: 0.861 \t Train Acc: 0.697\n",
      "\t [Batch 200 / 782] Train Loss: 0.852 \t Train Acc: 0.700\n",
      "\t [Batch 250 / 782] Train Loss: 0.844 \t Train Acc: 0.703\n",
      "\t [Batch 300 / 782] Train Loss: 0.843 \t Train Acc: 0.703\n",
      "\t [Batch 350 / 782] Train Loss: 0.843 \t Train Acc: 0.703\n",
      "\t [Batch 400 / 782] Train Loss: 0.839 \t Train Acc: 0.703\n",
      "\t [Batch 450 / 782] Train Loss: 0.836 \t Train Acc: 0.705\n",
      "\t [Batch 500 / 782] Train Loss: 0.832 \t Train Acc: 0.706\n",
      "\t [Batch 550 / 782] Train Loss: 0.829 \t Train Acc: 0.707\n",
      "\t [Batch 600 / 782] Train Loss: 0.825 \t Train Acc: 0.708\n",
      "\t [Batch 650 / 782] Train Loss: 0.822 \t Train Acc: 0.710\n",
      "\t [Batch 700 / 782] Train Loss: 0.817 \t Train Acc: 0.711\n",
      "\t [Batch 750 / 782] Train Loss: 0.816 \t Train Acc: 0.711\n",
      "Epoch 3:\t Train Loss: 0.815 \t Train Acc: 0.712\t Test Acc: 0.558\t Test Acc Flipped: 0.561\n",
      "Starting Epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f839dea8ea7647f9929669f8b95a2935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 0.701 \t Train Acc: 0.754\n",
      "\t [Batch 100 / 782] Train Loss: 0.707 \t Train Acc: 0.751\n",
      "\t [Batch 150 / 782] Train Loss: 0.713 \t Train Acc: 0.749\n",
      "\t [Batch 200 / 782] Train Loss: 0.707 \t Train Acc: 0.751\n",
      "\t [Batch 250 / 782] Train Loss: 0.709 \t Train Acc: 0.750\n",
      "\t [Batch 300 / 782] Train Loss: 0.706 \t Train Acc: 0.751\n",
      "\t [Batch 350 / 782] Train Loss: 0.705 \t Train Acc: 0.751\n",
      "\t [Batch 400 / 782] Train Loss: 0.705 \t Train Acc: 0.752\n",
      "\t [Batch 450 / 782] Train Loss: 0.704 \t Train Acc: 0.752\n",
      "\t [Batch 500 / 782] Train Loss: 0.704 \t Train Acc: 0.753\n",
      "\t [Batch 550 / 782] Train Loss: 0.701 \t Train Acc: 0.754\n",
      "\t [Batch 600 / 782] Train Loss: 0.700 \t Train Acc: 0.754\n",
      "\t [Batch 650 / 782] Train Loss: 0.703 \t Train Acc: 0.754\n",
      "\t [Batch 700 / 782] Train Loss: 0.701 \t Train Acc: 0.755\n",
      "\t [Batch 750 / 782] Train Loss: 0.701 \t Train Acc: 0.755\n",
      "Epoch 4:\t Train Loss: 0.699 \t Train Acc: 0.755\t Test Acc: 0.525\t Test Acc Flipped: 0.545\n",
      "Starting Epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7c9efc17364d14a2f8b1353d397f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 0.604 \t Train Acc: 0.789\n",
      "\t [Batch 100 / 782] Train Loss: 0.597 \t Train Acc: 0.792\n",
      "\t [Batch 150 / 782] Train Loss: 0.601 \t Train Acc: 0.790\n",
      "\t [Batch 200 / 782] Train Loss: 0.612 \t Train Acc: 0.787\n",
      "\t [Batch 250 / 782] Train Loss: 0.617 \t Train Acc: 0.787\n",
      "\t [Batch 300 / 782] Train Loss: 0.613 \t Train Acc: 0.788\n",
      "\t [Batch 350 / 782] Train Loss: 0.606 \t Train Acc: 0.790\n",
      "\t [Batch 400 / 782] Train Loss: 0.606 \t Train Acc: 0.790\n",
      "\t [Batch 450 / 782] Train Loss: 0.607 \t Train Acc: 0.790\n",
      "\t [Batch 500 / 782] Train Loss: 0.608 \t Train Acc: 0.789\n",
      "\t [Batch 550 / 782] Train Loss: 0.608 \t Train Acc: 0.788\n",
      "\t [Batch 600 / 782] Train Loss: 0.608 \t Train Acc: 0.789\n",
      "\t [Batch 650 / 782] Train Loss: 0.607 \t Train Acc: 0.789\n",
      "\t [Batch 700 / 782] Train Loss: 0.606 \t Train Acc: 0.789\n",
      "\t [Batch 750 / 782] Train Loss: 0.605 \t Train Acc: 0.790\n",
      "Epoch 5:\t Train Loss: 0.603 \t Train Acc: 0.790\t Test Acc: 0.672\t Test Acc Flipped: 0.678\n",
      "Starting Epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9162815cb01a46679689048a4203dd31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 0.529 \t Train Acc: 0.817\n",
      "\t [Batch 100 / 782] Train Loss: 0.510 \t Train Acc: 0.823\n",
      "\t [Batch 150 / 782] Train Loss: 0.504 \t Train Acc: 0.827\n",
      "\t [Batch 200 / 782] Train Loss: 0.515 \t Train Acc: 0.822\n",
      "\t [Batch 250 / 782] Train Loss: 0.514 \t Train Acc: 0.822\n",
      "\t [Batch 300 / 782] Train Loss: 0.518 \t Train Acc: 0.821\n",
      "\t [Batch 350 / 782] Train Loss: 0.520 \t Train Acc: 0.820\n",
      "\t [Batch 400 / 782] Train Loss: 0.525 \t Train Acc: 0.818\n",
      "\t [Batch 450 / 782] Train Loss: 0.524 \t Train Acc: 0.818\n",
      "\t [Batch 500 / 782] Train Loss: 0.522 \t Train Acc: 0.819\n",
      "\t [Batch 550 / 782] Train Loss: 0.523 \t Train Acc: 0.819\n",
      "\t [Batch 600 / 782] Train Loss: 0.521 \t Train Acc: 0.819\n",
      "\t [Batch 650 / 782] Train Loss: 0.523 \t Train Acc: 0.819\n",
      "\t [Batch 700 / 782] Train Loss: 0.522 \t Train Acc: 0.819\n",
      "\t [Batch 750 / 782] Train Loss: 0.522 \t Train Acc: 0.819\n",
      "Epoch 6:\t Train Loss: 0.524 \t Train Acc: 0.818\t Test Acc: 0.649\t Test Acc Flipped: 0.639\n",
      "Starting Epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76c461802e74130bab3f25cfe1fbade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 0.411 \t Train Acc: 0.857\n",
      "\t [Batch 100 / 782] Train Loss: 0.426 \t Train Acc: 0.855\n",
      "\t [Batch 150 / 782] Train Loss: 0.424 \t Train Acc: 0.855\n",
      "\t [Batch 200 / 782] Train Loss: 0.428 \t Train Acc: 0.853\n",
      "\t [Batch 250 / 782] Train Loss: 0.433 \t Train Acc: 0.851\n",
      "\t [Batch 300 / 782] Train Loss: 0.437 \t Train Acc: 0.848\n",
      "\t [Batch 350 / 782] Train Loss: 0.444 \t Train Acc: 0.847\n",
      "\t [Batch 400 / 782] Train Loss: 0.441 \t Train Acc: 0.848\n",
      "\t [Batch 450 / 782] Train Loss: 0.443 \t Train Acc: 0.847\n",
      "\t [Batch 500 / 782] Train Loss: 0.442 \t Train Acc: 0.847\n",
      "\t [Batch 550 / 782] Train Loss: 0.443 \t Train Acc: 0.847\n",
      "\t [Batch 600 / 782] Train Loss: 0.445 \t Train Acc: 0.846\n",
      "\t [Batch 650 / 782] Train Loss: 0.443 \t Train Acc: 0.847\n",
      "\t [Batch 700 / 782] Train Loss: 0.443 \t Train Acc: 0.847\n",
      "\t [Batch 750 / 782] Train Loss: 0.444 \t Train Acc: 0.846\n",
      "Epoch 7:\t Train Loss: 0.443 \t Train Acc: 0.847\t Test Acc: 0.660\t Test Acc Flipped: 0.652\n",
      "Starting Epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce53a5540444f9496bd69e77b01c350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 0.365 \t Train Acc: 0.877\n",
      "\t [Batch 100 / 782] Train Loss: 0.355 \t Train Acc: 0.877\n",
      "\t [Batch 150 / 782] Train Loss: 0.373 \t Train Acc: 0.870\n",
      "\t [Batch 200 / 782] Train Loss: 0.372 \t Train Acc: 0.870\n",
      "\t [Batch 250 / 782] Train Loss: 0.371 \t Train Acc: 0.870\n",
      "\t [Batch 300 / 782] Train Loss: 0.375 \t Train Acc: 0.868\n",
      "\t [Batch 350 / 782] Train Loss: 0.373 \t Train Acc: 0.869\n",
      "\t [Batch 400 / 782] Train Loss: 0.376 \t Train Acc: 0.868\n",
      "\t [Batch 450 / 782] Train Loss: 0.376 \t Train Acc: 0.868\n",
      "\t [Batch 500 / 782] Train Loss: 0.377 \t Train Acc: 0.868\n",
      "\t [Batch 550 / 782] Train Loss: 0.376 \t Train Acc: 0.868\n",
      "\t [Batch 600 / 782] Train Loss: 0.378 \t Train Acc: 0.868\n",
      "\t [Batch 650 / 782] Train Loss: 0.382 \t Train Acc: 0.866\n",
      "\t [Batch 700 / 782] Train Loss: 0.381 \t Train Acc: 0.867\n",
      "\t [Batch 750 / 782] Train Loss: 0.384 \t Train Acc: 0.866\n",
      "Epoch 8:\t Train Loss: 0.383 \t Train Acc: 0.866\t Test Acc: 0.737\t Test Acc Flipped: 0.739\n",
      "Starting Epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69371e0ea1cb40ed9ec7924f825f8e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 0.271 \t Train Acc: 0.905\n",
      "\t [Batch 100 / 782] Train Loss: 0.294 \t Train Acc: 0.898\n",
      "\t [Batch 150 / 782] Train Loss: 0.290 \t Train Acc: 0.899\n",
      "\t [Batch 200 / 782] Train Loss: 0.294 \t Train Acc: 0.896\n",
      "\t [Batch 250 / 782] Train Loss: 0.301 \t Train Acc: 0.895\n",
      "\t [Batch 300 / 782] Train Loss: 0.300 \t Train Acc: 0.895\n",
      "\t [Batch 350 / 782] Train Loss: 0.302 \t Train Acc: 0.895\n",
      "\t [Batch 400 / 782] Train Loss: 0.301 \t Train Acc: 0.895\n",
      "\t [Batch 450 / 782] Train Loss: 0.304 \t Train Acc: 0.895\n",
      "\t [Batch 500 / 782] Train Loss: 0.307 \t Train Acc: 0.894\n",
      "\t [Batch 550 / 782] Train Loss: 0.307 \t Train Acc: 0.894\n",
      "\t [Batch 600 / 782] Train Loss: 0.309 \t Train Acc: 0.893\n",
      "\t [Batch 650 / 782] Train Loss: 0.310 \t Train Acc: 0.892\n",
      "\t [Batch 700 / 782] Train Loss: 0.310 \t Train Acc: 0.892\n",
      "\t [Batch 750 / 782] Train Loss: 0.317 \t Train Acc: 0.890\n",
      "Epoch 9:\t Train Loss: 0.317 \t Train Acc: 0.890\t Test Acc: 0.668\t Test Acc Flipped: 0.672\n",
      "Starting Epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3589193c834d7bacf7eaacb024081b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 0.225 \t Train Acc: 0.925\n",
      "\t [Batch 100 / 782] Train Loss: 0.213 \t Train Acc: 0.928\n",
      "\t [Batch 150 / 782] Train Loss: 0.222 \t Train Acc: 0.925\n",
      "\t [Batch 200 / 782] Train Loss: 0.222 \t Train Acc: 0.925\n",
      "\t [Batch 250 / 782] Train Loss: 0.228 \t Train Acc: 0.923\n",
      "\t [Batch 300 / 782] Train Loss: 0.221 \t Train Acc: 0.926\n",
      "\t [Batch 350 / 782] Train Loss: 0.229 \t Train Acc: 0.923\n",
      "\t [Batch 400 / 782] Train Loss: 0.230 \t Train Acc: 0.922\n",
      "\t [Batch 450 / 782] Train Loss: 0.231 \t Train Acc: 0.922\n",
      "\t [Batch 500 / 782] Train Loss: 0.238 \t Train Acc: 0.920\n",
      "\t [Batch 550 / 782] Train Loss: 0.242 \t Train Acc: 0.919\n",
      "\t [Batch 600 / 782] Train Loss: 0.245 \t Train Acc: 0.918\n",
      "\t [Batch 650 / 782] Train Loss: 0.248 \t Train Acc: 0.917\n",
      "\t [Batch 700 / 782] Train Loss: 0.248 \t Train Acc: 0.917\n",
      "\t [Batch 750 / 782] Train Loss: 0.249 \t Train Acc: 0.916\n",
      "Epoch 10:\t Train Loss: 0.248 \t Train Acc: 0.916\t Test Acc: 0.682\t Test Acc Flipped: 0.674\n",
      "Starting Epoch 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255d195335934a6a8cefec367aa660a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 0.192 \t Train Acc: 0.939\n",
      "\t [Batch 100 / 782] Train Loss: 0.172 \t Train Acc: 0.943\n",
      "\t [Batch 150 / 782] Train Loss: 0.178 \t Train Acc: 0.942\n",
      "\t [Batch 200 / 782] Train Loss: 0.178 \t Train Acc: 0.943\n",
      "\t [Batch 250 / 782] Train Loss: 0.175 \t Train Acc: 0.944\n",
      "\t [Batch 300 / 782] Train Loss: 0.178 \t Train Acc: 0.942\n",
      "\t [Batch 350 / 782] Train Loss: 0.179 \t Train Acc: 0.942\n",
      "\t [Batch 400 / 782] Train Loss: 0.180 \t Train Acc: 0.941\n",
      "\t [Batch 450 / 782] Train Loss: 0.186 \t Train Acc: 0.939\n",
      "\t [Batch 500 / 782] Train Loss: 0.190 \t Train Acc: 0.938\n",
      "\t [Batch 550 / 782] Train Loss: 0.192 \t Train Acc: 0.937\n",
      "\t [Batch 600 / 782] Train Loss: 0.195 \t Train Acc: 0.936\n",
      "\t [Batch 650 / 782] Train Loss: 0.194 \t Train Acc: 0.936\n",
      "\t [Batch 700 / 782] Train Loss: 0.196 \t Train Acc: 0.935\n",
      "\t [Batch 750 / 782] Train Loss: 0.196 \t Train Acc: 0.935\n",
      "Epoch 11:\t Train Loss: 0.196 \t Train Acc: 0.935\t Test Acc: 0.744\t Test Acc Flipped: 0.749\n",
      "Starting Epoch 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a59ec2b4e64586afb9ded675ff31e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 0.106 \t Train Acc: 0.968\n",
      "\t [Batch 100 / 782] Train Loss: 0.100 \t Train Acc: 0.970\n",
      "\t [Batch 150 / 782] Train Loss: 0.103 \t Train Acc: 0.968\n",
      "\t [Batch 200 / 782] Train Loss: 0.100 \t Train Acc: 0.969\n",
      "\t [Batch 250 / 782] Train Loss: 0.100 \t Train Acc: 0.968\n",
      "\t [Batch 300 / 782] Train Loss: 0.098 \t Train Acc: 0.969\n",
      "\t [Batch 350 / 782] Train Loss: 0.095 \t Train Acc: 0.971\n",
      "\t [Batch 400 / 782] Train Loss: 0.097 \t Train Acc: 0.970\n",
      "\t [Batch 450 / 782] Train Loss: 0.104 \t Train Acc: 0.967\n",
      "\t [Batch 500 / 782] Train Loss: 0.103 \t Train Acc: 0.968\n",
      "\t [Batch 550 / 782] Train Loss: 0.105 \t Train Acc: 0.967\n",
      "\t [Batch 600 / 782] Train Loss: 0.111 \t Train Acc: 0.965\n",
      "\t [Batch 650 / 782] Train Loss: 0.113 \t Train Acc: 0.964\n",
      "\t [Batch 700 / 782] Train Loss: 0.115 \t Train Acc: 0.964\n",
      "\t [Batch 750 / 782] Train Loss: 0.117 \t Train Acc: 0.963\n",
      "Epoch 12:\t Train Loss: 0.119 \t Train Acc: 0.962\t Test Acc: 0.739\t Test Acc Flipped: 0.739\n",
      "Starting Epoch 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1095ead3acf84e88b27ad0ebe4bea270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 0.061 \t Train Acc: 0.983\n",
      "\t [Batch 100 / 782] Train Loss: 0.057 \t Train Acc: 0.985\n",
      "\t [Batch 150 / 782] Train Loss: 0.054 \t Train Acc: 0.986\n",
      "\t [Batch 200 / 782] Train Loss: 0.054 \t Train Acc: 0.986\n",
      "\t [Batch 250 / 782] Train Loss: 0.053 \t Train Acc: 0.986\n",
      "\t [Batch 300 / 782] Train Loss: 0.053 \t Train Acc: 0.986\n",
      "\t [Batch 350 / 782] Train Loss: 0.056 \t Train Acc: 0.985\n",
      "\t [Batch 400 / 782] Train Loss: 0.057 \t Train Acc: 0.984\n",
      "\t [Batch 450 / 782] Train Loss: 0.057 \t Train Acc: 0.984\n",
      "\t [Batch 500 / 782] Train Loss: 0.056 \t Train Acc: 0.984\n",
      "\t [Batch 550 / 782] Train Loss: 0.057 \t Train Acc: 0.984\n",
      "\t [Batch 600 / 782] Train Loss: 0.058 \t Train Acc: 0.984\n",
      "\t [Batch 650 / 782] Train Loss: 0.058 \t Train Acc: 0.984\n",
      "\t [Batch 700 / 782] Train Loss: 0.059 \t Train Acc: 0.983\n",
      "\t [Batch 750 / 782] Train Loss: 0.060 \t Train Acc: 0.983\n",
      "Epoch 13:\t Train Loss: 0.059 \t Train Acc: 0.983\t Test Acc: 0.795\t Test Acc Flipped: 0.786\n",
      "Starting Epoch 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c9bdaea1eb4d69b6c18dfa96a79827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 0.026 \t Train Acc: 0.995\n",
      "\t [Batch 100 / 782] Train Loss: 0.025 \t Train Acc: 0.996\n",
      "\t [Batch 150 / 782] Train Loss: 0.024 \t Train Acc: 0.996\n",
      "\t [Batch 200 / 782] Train Loss: 0.023 \t Train Acc: 0.996\n",
      "\t [Batch 250 / 782] Train Loss: 0.023 \t Train Acc: 0.996\n",
      "\t [Batch 300 / 782] Train Loss: 0.022 \t Train Acc: 0.997\n",
      "\t [Batch 350 / 782] Train Loss: 0.024 \t Train Acc: 0.996\n",
      "\t [Batch 400 / 782] Train Loss: 0.024 \t Train Acc: 0.996\n",
      "\t [Batch 450 / 782] Train Loss: 0.024 \t Train Acc: 0.996\n",
      "\t [Batch 500 / 782] Train Loss: 0.024 \t Train Acc: 0.996\n",
      "\t [Batch 550 / 782] Train Loss: 0.024 \t Train Acc: 0.996\n",
      "\t [Batch 600 / 782] Train Loss: 0.023 \t Train Acc: 0.996\n",
      "\t [Batch 650 / 782] Train Loss: 0.023 \t Train Acc: 0.996\n",
      "\t [Batch 700 / 782] Train Loss: 0.023 \t Train Acc: 0.996\n",
      "\t [Batch 750 / 782] Train Loss: 0.023 \t Train Acc: 0.996\n",
      "Epoch 14:\t Train Loss: 0.023 \t Train Acc: 0.996\t Test Acc: 0.792\t Test Acc Flipped: 0.799\n",
      "Starting Epoch 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcbbbff2db70475a8ddcab95c964a832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 0.012 \t Train Acc: 0.999\n",
      "\t [Batch 100 / 782] Train Loss: 0.011 \t Train Acc: 0.999\n",
      "\t [Batch 150 / 782] Train Loss: 0.011 \t Train Acc: 0.999\n",
      "\t [Batch 200 / 782] Train Loss: 0.010 \t Train Acc: 0.999\n",
      "\t [Batch 250 / 782] Train Loss: 0.010 \t Train Acc: 0.999\n",
      "\t [Batch 300 / 782] Train Loss: 0.010 \t Train Acc: 0.999\n",
      "\t [Batch 350 / 782] Train Loss: 0.010 \t Train Acc: 0.999\n",
      "\t [Batch 400 / 782] Train Loss: 0.010 \t Train Acc: 0.999\n",
      "\t [Batch 450 / 782] Train Loss: 0.010 \t Train Acc: 0.999\n",
      "\t [Batch 500 / 782] Train Loss: 0.009 \t Train Acc: 0.999\n",
      "\t [Batch 550 / 782] Train Loss: 0.009 \t Train Acc: 0.999\n",
      "\t [Batch 600 / 782] Train Loss: 0.009 \t Train Acc: 0.999\n",
      "\t [Batch 650 / 782] Train Loss: 0.009 \t Train Acc: 0.999\n",
      "\t [Batch 700 / 782] Train Loss: 0.009 \t Train Acc: 0.999\n",
      "\t [Batch 750 / 782] Train Loss: 0.009 \t Train Acc: 0.999\n",
      "Epoch 15:\t Train Loss: 0.009 \t Train Acc: 1.000\t Test Acc: 0.827\t Test Acc Flipped: 0.827\n",
      "Starting Epoch 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9cef51b828844ceb2f969d28b5c4282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 0.005 \t Train Acc: 1.000\n",
      "\t [Batch 100 / 782] Train Loss: 0.005 \t Train Acc: 1.000\n",
      "\t [Batch 150 / 782] Train Loss: 0.005 \t Train Acc: 1.000\n",
      "\t [Batch 200 / 782] Train Loss: 0.005 \t Train Acc: 1.000\n",
      "\t [Batch 250 / 782] Train Loss: 0.005 \t Train Acc: 1.000\n",
      "\t [Batch 300 / 782] Train Loss: 0.005 \t Train Acc: 1.000\n",
      "\t [Batch 350 / 782] Train Loss: 0.005 \t Train Acc: 1.000\n",
      "\t [Batch 400 / 782] Train Loss: 0.004 \t Train Acc: 1.000\n",
      "\t [Batch 450 / 782] Train Loss: 0.004 \t Train Acc: 1.000\n",
      "\t [Batch 500 / 782] Train Loss: 0.004 \t Train Acc: 1.000\n",
      "\t [Batch 550 / 782] Train Loss: 0.004 \t Train Acc: 1.000\n",
      "\t [Batch 600 / 782] Train Loss: 0.004 \t Train Acc: 1.000\n",
      "\t [Batch 650 / 782] Train Loss: 0.004 \t Train Acc: 1.000\n",
      "\t [Batch 700 / 782] Train Loss: 0.004 \t Train Acc: 1.000\n",
      "\t [Batch 750 / 782] Train Loss: 0.004 \t Train Acc: 1.000\n",
      "Epoch 16:\t Train Loss: 0.004 \t Train Acc: 1.000\t Test Acc: 0.838\t Test Acc Flipped: 0.833\n",
      "Starting Epoch 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8755b8cb3a47dbb6da5d1a986e8a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 100 / 782] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 150 / 782] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 200 / 782] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 250 / 782] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 300 / 782] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 350 / 782] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 400 / 782] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 450 / 782] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 500 / 782] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 550 / 782] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 600 / 782] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 650 / 782] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 700 / 782] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "\t [Batch 750 / 782] Train Loss: 0.003 \t Train Acc: 1.000\n",
      "Epoch 17:\t Train Loss: 0.003 \t Train Acc: 1.000\t Test Acc: 0.840\t Test Acc Flipped: 0.837\n",
      "Starting Epoch 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4ab10fe79f4da68e15f3a44ebf7d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 100 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 150 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 200 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 250 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 300 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 350 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 400 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 450 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 500 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 550 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 600 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 650 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 700 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 750 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "Epoch 18:\t Train Loss: 0.002 \t Train Acc: 1.000\t Test Acc: 0.838\t Test Acc Flipped: 0.832\n",
      "Starting Epoch 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f1dce7f49345f3938b21b6c186cc27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [Batch 50 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 100 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 150 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 200 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 250 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 300 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 350 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 400 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 450 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 500 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 550 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 600 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 650 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 700 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "\t [Batch 750 / 782] Train Loss: 0.002 \t Train Acc: 1.000\n",
      "Epoch 19:\t Train Loss: 0.002 \t Train Acc: 1.000\t Test Acc: 0.841\t Test Acc Flipped: 0.836\n"
     ]
    }
   ],
   "source": [
    "model_combined = make_cnn()\n",
    "opt = torch.optim.SGD(model_combined.parameters(), lr=0.1)\n",
    "epochs = 20\n",
    "for i in range(epochs):\n",
    "    print(f'Starting Epoch {i}')\n",
    "    train_loss, train_acc = train_epoch(model_combined, train_combined_dl, opt)\n",
    "    test_loss, test_acc = evaluate(model_combined, test_dl)\n",
    "    test_loss_flipped, test_acc_flipped = evaluate(model_combined, test_dl_flipped)\n",
    "    \n",
    "    print(f'Epoch {i}:\\t Train Loss: {train_loss:.3f} \\t Train Acc: {train_acc:.3f}\\t ' + \n",
    "          f'Test Acc: {test_acc:.3f}\\t Test Acc Flipped: {test_acc_flipped:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomizing Labels in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_labels(labels, p):\n",
    "    rand = torch.where(torch.rand(size=labels.shape) < p, torch.tensor([1]), torch.tensor([0])) \n",
    "    return rand * torch.randint(low=0, high=10, size=labels.shape) + (1 - rand) * labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_vals = [0, 0.5, 1.0]\n",
    "training_labels = []\n",
    "testing_labels = []\n",
    "for p in p_vals:\n",
    "    training_labels.append(randomize_labels(Y_tr, p))\n",
    "    testing_labels.append(randomize_labels(Y_te, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\t Train Loss: 1.726 \t Train Acc: 0.372\t Test Acc: 0.308\n",
      "Epoch 2:\t Train Loss: 1.288 \t Train Acc: 0.536\t Test Acc: 0.427\n",
      "Epoch 4:\t Train Loss: 1.127 \t Train Acc: 0.599\t Test Acc: 0.537\n",
      "Epoch 6:\t Train Loss: 1.045 \t Train Acc: 0.630\t Test Acc: 0.559\n",
      "Epoch 8:\t Train Loss: 0.995 \t Train Acc: 0.647\t Test Acc: 0.620\n",
      "Epoch 10:\t Train Loss: 0.961 \t Train Acc: 0.659\t Test Acc: 0.620\n",
      "Epoch 12:\t Train Loss: 0.934 \t Train Acc: 0.671\t Test Acc: 0.578\n",
      "Epoch 14:\t Train Loss: 0.918 \t Train Acc: 0.676\t Test Acc: 0.632\n",
      "Epoch 16:\t Train Loss: 0.896 \t Train Acc: 0.683\t Test Acc: 0.641\n",
      "Epoch 18:\t Train Loss: 0.889 \t Train Acc: 0.684\t Test Acc: 0.642\n",
      "Epoch 20:\t Train Loss: 0.873 \t Train Acc: 0.691\t Test Acc: 0.635\n",
      "Epoch 22:\t Train Loss: 0.862 \t Train Acc: 0.695\t Test Acc: 0.638\n",
      "Epoch 24:\t Train Loss: 0.854 \t Train Acc: 0.697\t Test Acc: 0.649\n",
      "Epoch 26:\t Train Loss: 0.847 \t Train Acc: 0.700\t Test Acc: 0.636\n",
      "Epoch 28:\t Train Loss: 0.839 \t Train Acc: 0.703\t Test Acc: 0.613\n",
      "Epoch 30:\t Train Loss: 0.830 \t Train Acc: 0.705\t Test Acc: 0.652\n",
      "Epoch 32:\t Train Loss: 0.822 \t Train Acc: 0.709\t Test Acc: 0.656\n",
      "Epoch 34:\t Train Loss: 0.818 \t Train Acc: 0.710\t Test Acc: 0.642\n",
      "Epoch 36:\t Train Loss: 0.812 \t Train Acc: 0.712\t Test Acc: 0.626\n",
      "Epoch 38:\t Train Loss: 0.808 \t Train Acc: 0.715\t Test Acc: 0.658\n",
      "Epoch 40:\t Train Loss: 0.804 \t Train Acc: 0.715\t Test Acc: 0.658\n",
      "Epoch 42:\t Train Loss: 0.798 \t Train Acc: 0.717\t Test Acc: 0.658\n",
      "Epoch 44:\t Train Loss: 0.795 \t Train Acc: 0.719\t Test Acc: 0.658\n",
      "Epoch 46:\t Train Loss: 0.794 \t Train Acc: 0.716\t Test Acc: 0.649\n",
      "Epoch 48:\t Train Loss: 0.788 \t Train Acc: 0.721\t Test Acc: 0.644\n",
      "Epoch 50:\t Train Loss: 0.784 \t Train Acc: 0.722\t Test Acc: 0.651\n",
      "Epoch 52:\t Train Loss: 0.781 \t Train Acc: 0.723\t Test Acc: 0.644\n",
      "Epoch 54:\t Train Loss: 0.778 \t Train Acc: 0.724\t Test Acc: 0.643\n",
      "Epoch 56:\t Train Loss: 0.776 \t Train Acc: 0.723\t Test Acc: 0.658\n",
      "Epoch 58:\t Train Loss: 0.771 \t Train Acc: 0.725\t Test Acc: 0.654\n",
      "Epoch 60:\t Train Loss: 0.768 \t Train Acc: 0.727\t Test Acc: 0.657\n",
      "Epoch 62:\t Train Loss: 0.765 \t Train Acc: 0.729\t Test Acc: 0.647\n",
      "Epoch 64:\t Train Loss: 0.765 \t Train Acc: 0.728\t Test Acc: 0.651\n",
      "Epoch 66:\t Train Loss: 0.761 \t Train Acc: 0.729\t Test Acc: 0.650\n",
      "Epoch 68:\t Train Loss: 0.760 \t Train Acc: 0.728\t Test Acc: 0.656\n",
      "Epoch 70:\t Train Loss: 0.759 \t Train Acc: 0.731\t Test Acc: 0.651\n",
      "Epoch 72:\t Train Loss: 0.755 \t Train Acc: 0.731\t Test Acc: 0.655\n",
      "Epoch 74:\t Train Loss: 0.755 \t Train Acc: 0.731\t Test Acc: 0.651\n",
      "Epoch 76:\t Train Loss: 0.753 \t Train Acc: 0.733\t Test Acc: 0.654\n",
      "Epoch 78:\t Train Loss: 0.750 \t Train Acc: 0.734\t Test Acc: 0.652\n",
      "Epoch 80:\t Train Loss: 0.746 \t Train Acc: 0.735\t Test Acc: 0.647\n",
      "Epoch 82:\t Train Loss: 0.745 \t Train Acc: 0.734\t Test Acc: 0.656\n",
      "Epoch 84:\t Train Loss: 0.744 \t Train Acc: 0.734\t Test Acc: 0.652\n",
      "Epoch 86:\t Train Loss: 0.743 \t Train Acc: 0.736\t Test Acc: 0.650\n",
      "Epoch 88:\t Train Loss: 0.739 \t Train Acc: 0.737\t Test Acc: 0.661\n",
      "Epoch 90:\t Train Loss: 0.740 \t Train Acc: 0.736\t Test Acc: 0.657\n",
      "Epoch 92:\t Train Loss: 0.738 \t Train Acc: 0.737\t Test Acc: 0.662\n",
      "Epoch 94:\t Train Loss: 0.736 \t Train Acc: 0.737\t Test Acc: 0.657\n",
      "Epoch 96:\t Train Loss: 0.735 \t Train Acc: 0.738\t Test Acc: 0.659\n",
      "Epoch 98:\t Train Loss: 0.733 \t Train Acc: 0.741\t Test Acc: 0.658\n",
      "Epoch 100:\t Train Loss: 0.733 \t Train Acc: 0.741\t Test Acc: 0.657\n",
      "Epoch 102:\t Train Loss: 0.730 \t Train Acc: 0.741\t Test Acc: 0.655\n",
      "Epoch 104:\t Train Loss: 0.728 \t Train Acc: 0.740\t Test Acc: 0.663\n",
      "Epoch 106:\t Train Loss: 0.728 \t Train Acc: 0.740\t Test Acc: 0.653\n",
      "Epoch 108:\t Train Loss: 0.727 \t Train Acc: 0.741\t Test Acc: 0.655\n",
      "Epoch 110:\t Train Loss: 0.724 \t Train Acc: 0.742\t Test Acc: 0.662\n",
      "Epoch 112:\t Train Loss: 0.724 \t Train Acc: 0.742\t Test Acc: 0.658\n",
      "Epoch 114:\t Train Loss: 0.723 \t Train Acc: 0.742\t Test Acc: 0.658\n",
      "Epoch 116:\t Train Loss: 0.721 \t Train Acc: 0.742\t Test Acc: 0.656\n",
      "Epoch 118:\t Train Loss: 0.718 \t Train Acc: 0.745\t Test Acc: 0.652\n",
      "Epoch 120:\t Train Loss: 0.719 \t Train Acc: 0.744\t Test Acc: 0.660\n",
      "Epoch 122:\t Train Loss: 0.716 \t Train Acc: 0.743\t Test Acc: 0.656\n",
      "Epoch 124:\t Train Loss: 0.715 \t Train Acc: 0.747\t Test Acc: 0.660\n",
      "Epoch 126:\t Train Loss: 0.714 \t Train Acc: 0.744\t Test Acc: 0.654\n",
      "Epoch 128:\t Train Loss: 0.714 \t Train Acc: 0.746\t Test Acc: 0.653\n",
      "Epoch 130:\t Train Loss: 0.713 \t Train Acc: 0.747\t Test Acc: 0.657\n",
      "Epoch 132:\t Train Loss: 0.713 \t Train Acc: 0.745\t Test Acc: 0.660\n",
      "Epoch 134:\t Train Loss: 0.709 \t Train Acc: 0.748\t Test Acc: 0.661\n",
      "Epoch 136:\t Train Loss: 0.711 \t Train Acc: 0.745\t Test Acc: 0.657\n",
      "Epoch 138:\t Train Loss: 0.708 \t Train Acc: 0.747\t Test Acc: 0.661\n",
      "Epoch 140:\t Train Loss: 0.709 \t Train Acc: 0.748\t Test Acc: 0.656\n",
      "Epoch 142:\t Train Loss: 0.708 \t Train Acc: 0.747\t Test Acc: 0.653\n",
      "Epoch 144:\t Train Loss: 0.705 \t Train Acc: 0.749\t Test Acc: 0.660\n",
      "Epoch 146:\t Train Loss: 0.706 \t Train Acc: 0.748\t Test Acc: 0.661\n",
      "Epoch 148:\t Train Loss: 0.705 \t Train Acc: 0.749\t Test Acc: 0.656\n",
      "Epoch 150:\t Train Loss: 0.706 \t Train Acc: 0.748\t Test Acc: 0.656\n",
      "Epoch 152:\t Train Loss: 0.705 \t Train Acc: 0.750\t Test Acc: 0.653\n",
      "Epoch 154:\t Train Loss: 0.704 \t Train Acc: 0.750\t Test Acc: 0.655\n",
      "Epoch 156:\t Train Loss: 0.703 \t Train Acc: 0.749\t Test Acc: 0.660\n",
      "Epoch 158:\t Train Loss: 0.701 \t Train Acc: 0.751\t Test Acc: 0.657\n",
      "Epoch 160:\t Train Loss: 0.702 \t Train Acc: 0.749\t Test Acc: 0.659\n",
      "Epoch 162:\t Train Loss: 0.701 \t Train Acc: 0.750\t Test Acc: 0.659\n",
      "Epoch 164:\t Train Loss: 0.698 \t Train Acc: 0.751\t Test Acc: 0.657\n",
      "Epoch 166:\t Train Loss: 0.698 \t Train Acc: 0.752\t Test Acc: 0.657\n",
      "Epoch 168:\t Train Loss: 0.698 \t Train Acc: 0.752\t Test Acc: 0.658\n",
      "Epoch 170:\t Train Loss: 0.697 \t Train Acc: 0.752\t Test Acc: 0.661\n",
      "Epoch 172:\t Train Loss: 0.697 \t Train Acc: 0.752\t Test Acc: 0.659\n",
      "Epoch 174:\t Train Loss: 0.697 \t Train Acc: 0.751\t Test Acc: 0.661\n",
      "Epoch 176:\t Train Loss: 0.694 \t Train Acc: 0.754\t Test Acc: 0.660\n",
      "Epoch 178:\t Train Loss: 0.693 \t Train Acc: 0.753\t Test Acc: 0.657\n",
      "Epoch 180:\t Train Loss: 0.693 \t Train Acc: 0.754\t Test Acc: 0.652\n",
      "Epoch 182:\t Train Loss: 0.691 \t Train Acc: 0.755\t Test Acc: 0.659\n",
      "Epoch 184:\t Train Loss: 0.691 \t Train Acc: 0.754\t Test Acc: 0.656\n",
      "Epoch 186:\t Train Loss: 0.692 \t Train Acc: 0.753\t Test Acc: 0.660\n",
      "Epoch 188:\t Train Loss: 0.690 \t Train Acc: 0.755\t Test Acc: 0.657\n",
      "Epoch 190:\t Train Loss: 0.688 \t Train Acc: 0.754\t Test Acc: 0.656\n",
      "Epoch 192:\t Train Loss: 0.690 \t Train Acc: 0.754\t Test Acc: 0.658\n",
      "Epoch 194:\t Train Loss: 0.689 \t Train Acc: 0.756\t Test Acc: 0.659\n",
      "Epoch 196:\t Train Loss: 0.688 \t Train Acc: 0.754\t Test Acc: 0.659\n",
      "Epoch 198:\t Train Loss: 0.688 \t Train Acc: 0.756\t Test Acc: 0.653\n",
      "Epoch 0:\t Train Loss: 2.220 \t Train Acc: 0.199\t Test Acc: 0.229\n",
      "Epoch 2:\t Train Loss: 2.124 \t Train Acc: 0.272\t Test Acc: 0.280\n",
      "Epoch 4:\t Train Loss: 2.092 \t Train Acc: 0.293\t Test Acc: 0.260\n",
      "Epoch 6:\t Train Loss: 2.071 \t Train Acc: 0.305\t Test Acc: 0.278\n",
      "Epoch 8:\t Train Loss: 2.052 \t Train Acc: 0.321\t Test Acc: 0.284\n",
      "Epoch 10:\t Train Loss: 2.038 \t Train Acc: 0.326\t Test Acc: 0.314\n",
      "Epoch 12:\t Train Loss: 2.025 \t Train Acc: 0.333\t Test Acc: 0.276\n",
      "Epoch 14:\t Train Loss: 2.013 \t Train Acc: 0.338\t Test Acc: 0.315\n",
      "Epoch 16:\t Train Loss: 2.006 \t Train Acc: 0.345\t Test Acc: 0.312\n",
      "Epoch 18:\t Train Loss: 1.997 \t Train Acc: 0.349\t Test Acc: 0.321\n",
      "Epoch 20:\t Train Loss: 1.989 \t Train Acc: 0.349\t Test Acc: 0.301\n",
      "Epoch 22:\t Train Loss: 1.983 \t Train Acc: 0.353\t Test Acc: 0.314\n",
      "Epoch 24:\t Train Loss: 1.977 \t Train Acc: 0.354\t Test Acc: 0.324\n",
      "Epoch 26:\t Train Loss: 1.972 \t Train Acc: 0.356\t Test Acc: 0.319\n",
      "Epoch 28:\t Train Loss: 1.966 \t Train Acc: 0.360\t Test Acc: 0.317\n",
      "Epoch 30:\t Train Loss: 1.963 \t Train Acc: 0.357\t Test Acc: 0.315\n",
      "Epoch 32:\t Train Loss: 1.957 \t Train Acc: 0.361\t Test Acc: 0.305\n",
      "Epoch 34:\t Train Loss: 1.954 \t Train Acc: 0.363\t Test Acc: 0.310\n",
      "Epoch 36:\t Train Loss: 1.950 \t Train Acc: 0.365\t Test Acc: 0.310\n",
      "Epoch 38:\t Train Loss: 1.946 \t Train Acc: 0.364\t Test Acc: 0.318\n",
      "Epoch 40:\t Train Loss: 1.942 \t Train Acc: 0.366\t Test Acc: 0.305\n",
      "Epoch 42:\t Train Loss: 1.939 \t Train Acc: 0.367\t Test Acc: 0.291\n",
      "Epoch 44:\t Train Loss: 1.937 \t Train Acc: 0.366\t Test Acc: 0.308\n",
      "Epoch 46:\t Train Loss: 1.933 \t Train Acc: 0.371\t Test Acc: 0.314\n",
      "Epoch 48:\t Train Loss: 1.930 \t Train Acc: 0.370\t Test Acc: 0.316\n",
      "Epoch 50:\t Train Loss: 1.929 \t Train Acc: 0.370\t Test Acc: 0.303\n",
      "Epoch 52:\t Train Loss: 1.926 \t Train Acc: 0.371\t Test Acc: 0.309\n",
      "Epoch 54:\t Train Loss: 1.923 \t Train Acc: 0.372\t Test Acc: 0.308\n",
      "Epoch 56:\t Train Loss: 1.923 \t Train Acc: 0.370\t Test Acc: 0.315\n",
      "Epoch 58:\t Train Loss: 1.919 \t Train Acc: 0.374\t Test Acc: 0.311\n",
      "Epoch 60:\t Train Loss: 1.918 \t Train Acc: 0.372\t Test Acc: 0.309\n",
      "Epoch 62:\t Train Loss: 1.916 \t Train Acc: 0.374\t Test Acc: 0.307\n",
      "Epoch 64:\t Train Loss: 1.914 \t Train Acc: 0.375\t Test Acc: 0.313\n",
      "Epoch 66:\t Train Loss: 1.913 \t Train Acc: 0.374\t Test Acc: 0.310\n",
      "Epoch 68:\t Train Loss: 1.911 \t Train Acc: 0.374\t Test Acc: 0.315\n",
      "Epoch 70:\t Train Loss: 1.909 \t Train Acc: 0.374\t Test Acc: 0.312\n",
      "Epoch 72:\t Train Loss: 1.908 \t Train Acc: 0.376\t Test Acc: 0.310\n",
      "Epoch 74:\t Train Loss: 1.906 \t Train Acc: 0.376\t Test Acc: 0.307\n",
      "Epoch 76:\t Train Loss: 1.904 \t Train Acc: 0.377\t Test Acc: 0.309\n",
      "Epoch 78:\t Train Loss: 1.903 \t Train Acc: 0.377\t Test Acc: 0.313\n",
      "Epoch 80:\t Train Loss: 1.901 \t Train Acc: 0.379\t Test Acc: 0.317\n",
      "Epoch 82:\t Train Loss: 1.899 \t Train Acc: 0.379\t Test Acc: 0.305\n",
      "Epoch 84:\t Train Loss: 1.899 \t Train Acc: 0.380\t Test Acc: 0.308\n",
      "Epoch 86:\t Train Loss: 1.898 \t Train Acc: 0.379\t Test Acc: 0.302\n",
      "Epoch 88:\t Train Loss: 1.896 \t Train Acc: 0.380\t Test Acc: 0.311\n",
      "Epoch 90:\t Train Loss: 1.896 \t Train Acc: 0.379\t Test Acc: 0.303\n",
      "Epoch 92:\t Train Loss: 1.894 \t Train Acc: 0.380\t Test Acc: 0.308\n",
      "Epoch 94:\t Train Loss: 1.893 \t Train Acc: 0.381\t Test Acc: 0.310\n",
      "Epoch 96:\t Train Loss: 1.892 \t Train Acc: 0.381\t Test Acc: 0.305\n",
      "Epoch 98:\t Train Loss: 1.892 \t Train Acc: 0.382\t Test Acc: 0.306\n",
      "Epoch 100:\t Train Loss: 1.890 \t Train Acc: 0.381\t Test Acc: 0.307\n",
      "Epoch 102:\t Train Loss: 1.888 \t Train Acc: 0.383\t Test Acc: 0.307\n",
      "Epoch 104:\t Train Loss: 1.889 \t Train Acc: 0.382\t Test Acc: 0.310\n",
      "Epoch 106:\t Train Loss: 1.888 \t Train Acc: 0.383\t Test Acc: 0.304\n",
      "Epoch 108:\t Train Loss: 1.886 \t Train Acc: 0.381\t Test Acc: 0.308\n",
      "Epoch 110:\t Train Loss: 1.885 \t Train Acc: 0.383\t Test Acc: 0.304\n",
      "Epoch 112:\t Train Loss: 1.885 \t Train Acc: 0.382\t Test Acc: 0.310\n",
      "Epoch 114:\t Train Loss: 1.884 \t Train Acc: 0.383\t Test Acc: 0.306\n",
      "Epoch 116:\t Train Loss: 1.881 \t Train Acc: 0.384\t Test Acc: 0.301\n",
      "Epoch 118:\t Train Loss: 1.883 \t Train Acc: 0.384\t Test Acc: 0.307\n",
      "Epoch 120:\t Train Loss: 1.882 \t Train Acc: 0.385\t Test Acc: 0.304\n",
      "Epoch 122:\t Train Loss: 1.879 \t Train Acc: 0.384\t Test Acc: 0.306\n",
      "Epoch 124:\t Train Loss: 1.880 \t Train Acc: 0.384\t Test Acc: 0.308\n",
      "Epoch 126:\t Train Loss: 1.879 \t Train Acc: 0.385\t Test Acc: 0.311\n",
      "Epoch 128:\t Train Loss: 1.878 \t Train Acc: 0.385\t Test Acc: 0.301\n",
      "Epoch 130:\t Train Loss: 1.879 \t Train Acc: 0.385\t Test Acc: 0.307\n",
      "Epoch 132:\t Train Loss: 1.877 \t Train Acc: 0.386\t Test Acc: 0.309\n",
      "Epoch 134:\t Train Loss: 1.876 \t Train Acc: 0.384\t Test Acc: 0.305\n",
      "Epoch 136:\t Train Loss: 1.876 \t Train Acc: 0.385\t Test Acc: 0.309\n",
      "Epoch 138:\t Train Loss: 1.874 \t Train Acc: 0.386\t Test Acc: 0.302\n",
      "Epoch 140:\t Train Loss: 1.875 \t Train Acc: 0.387\t Test Acc: 0.308\n",
      "Epoch 142:\t Train Loss: 1.873 \t Train Acc: 0.386\t Test Acc: 0.309\n",
      "Epoch 144:\t Train Loss: 1.873 \t Train Acc: 0.386\t Test Acc: 0.303\n",
      "Epoch 146:\t Train Loss: 1.873 \t Train Acc: 0.387\t Test Acc: 0.305\n",
      "Epoch 148:\t Train Loss: 1.872 \t Train Acc: 0.387\t Test Acc: 0.307\n",
      "Epoch 150:\t Train Loss: 1.872 \t Train Acc: 0.387\t Test Acc: 0.304\n",
      "Epoch 152:\t Train Loss: 1.871 \t Train Acc: 0.387\t Test Acc: 0.309\n",
      "Epoch 154:\t Train Loss: 1.870 \t Train Acc: 0.387\t Test Acc: 0.306\n",
      "Epoch 156:\t Train Loss: 1.870 \t Train Acc: 0.387\t Test Acc: 0.303\n",
      "Epoch 158:\t Train Loss: 1.870 \t Train Acc: 0.389\t Test Acc: 0.309\n",
      "Epoch 160:\t Train Loss: 1.869 \t Train Acc: 0.388\t Test Acc: 0.301\n",
      "Epoch 162:\t Train Loss: 1.870 \t Train Acc: 0.388\t Test Acc: 0.303\n",
      "Epoch 164:\t Train Loss: 1.867 \t Train Acc: 0.389\t Test Acc: 0.306\n",
      "Epoch 166:\t Train Loss: 1.867 \t Train Acc: 0.389\t Test Acc: 0.304\n",
      "Epoch 168:\t Train Loss: 1.867 \t Train Acc: 0.388\t Test Acc: 0.308\n",
      "Epoch 170:\t Train Loss: 1.866 \t Train Acc: 0.387\t Test Acc: 0.302\n",
      "Epoch 172:\t Train Loss: 1.866 \t Train Acc: 0.389\t Test Acc: 0.306\n",
      "Epoch 174:\t Train Loss: 1.865 \t Train Acc: 0.387\t Test Acc: 0.306\n",
      "Epoch 176:\t Train Loss: 1.865 \t Train Acc: 0.388\t Test Acc: 0.306\n",
      "Epoch 178:\t Train Loss: 1.864 \t Train Acc: 0.391\t Test Acc: 0.306\n",
      "Epoch 180:\t Train Loss: 1.862 \t Train Acc: 0.390\t Test Acc: 0.306\n",
      "Epoch 182:\t Train Loss: 1.864 \t Train Acc: 0.390\t Test Acc: 0.308\n",
      "Epoch 184:\t Train Loss: 1.863 \t Train Acc: 0.388\t Test Acc: 0.304\n",
      "Epoch 186:\t Train Loss: 1.862 \t Train Acc: 0.389\t Test Acc: 0.303\n",
      "Epoch 188:\t Train Loss: 1.862 \t Train Acc: 0.390\t Test Acc: 0.303\n",
      "Epoch 190:\t Train Loss: 1.861 \t Train Acc: 0.390\t Test Acc: 0.304\n",
      "Epoch 192:\t Train Loss: 1.861 \t Train Acc: 0.389\t Test Acc: 0.307\n",
      "Epoch 194:\t Train Loss: 1.861 \t Train Acc: 0.389\t Test Acc: 0.300\n",
      "Epoch 196:\t Train Loss: 1.859 \t Train Acc: 0.391\t Test Acc: 0.304\n",
      "Epoch 198:\t Train Loss: 1.861 \t Train Acc: 0.390\t Test Acc: 0.308\n",
      "Epoch 0:\t Train Loss: 2.323 \t Train Acc: 0.098\t Test Acc: 0.096\n",
      "Epoch 2:\t Train Loss: 2.305 \t Train Acc: 0.104\t Test Acc: 0.102\n",
      "Epoch 4:\t Train Loss: 2.303 \t Train Acc: 0.105\t Test Acc: 0.103\n",
      "Epoch 6:\t Train Loss: 2.303 \t Train Acc: 0.107\t Test Acc: 0.100\n",
      "Epoch 8:\t Train Loss: 2.301 \t Train Acc: 0.109\t Test Acc: 0.101\n",
      "Epoch 10:\t Train Loss: 2.301 \t Train Acc: 0.109\t Test Acc: 0.101\n",
      "Epoch 12:\t Train Loss: 2.299 \t Train Acc: 0.113\t Test Acc: 0.102\n",
      "Epoch 14:\t Train Loss: 2.298 \t Train Acc: 0.114\t Test Acc: 0.101\n",
      "Epoch 16:\t Train Loss: 2.297 \t Train Acc: 0.117\t Test Acc: 0.100\n",
      "Epoch 18:\t Train Loss: 2.295 \t Train Acc: 0.119\t Test Acc: 0.103\n",
      "Epoch 20:\t Train Loss: 2.294 \t Train Acc: 0.122\t Test Acc: 0.101\n",
      "Epoch 22:\t Train Loss: 2.292 \t Train Acc: 0.126\t Test Acc: 0.095\n",
      "Epoch 24:\t Train Loss: 2.290 \t Train Acc: 0.128\t Test Acc: 0.101\n",
      "Epoch 26:\t Train Loss: 2.287 \t Train Acc: 0.132\t Test Acc: 0.099\n",
      "Epoch 28:\t Train Loss: 2.285 \t Train Acc: 0.133\t Test Acc: 0.095\n",
      "Epoch 30:\t Train Loss: 2.284 \t Train Acc: 0.134\t Test Acc: 0.098\n",
      "Epoch 32:\t Train Loss: 2.281 \t Train Acc: 0.138\t Test Acc: 0.097\n",
      "Epoch 34:\t Train Loss: 2.278 \t Train Acc: 0.139\t Test Acc: 0.100\n",
      "Epoch 36:\t Train Loss: 2.276 \t Train Acc: 0.142\t Test Acc: 0.104\n",
      "Epoch 38:\t Train Loss: 2.272 \t Train Acc: 0.145\t Test Acc: 0.099\n",
      "Epoch 40:\t Train Loss: 2.271 \t Train Acc: 0.146\t Test Acc: 0.098\n",
      "Epoch 42:\t Train Loss: 2.268 \t Train Acc: 0.147\t Test Acc: 0.099\n",
      "Epoch 44:\t Train Loss: 2.265 \t Train Acc: 0.148\t Test Acc: 0.098\n",
      "Epoch 46:\t Train Loss: 2.263 \t Train Acc: 0.151\t Test Acc: 0.094\n",
      "Epoch 48:\t Train Loss: 2.260 \t Train Acc: 0.154\t Test Acc: 0.101\n",
      "Epoch 50:\t Train Loss: 2.257 \t Train Acc: 0.152\t Test Acc: 0.102\n",
      "Epoch 52:\t Train Loss: 2.253 \t Train Acc: 0.159\t Test Acc: 0.101\n",
      "Epoch 54:\t Train Loss: 2.251 \t Train Acc: 0.159\t Test Acc: 0.097\n",
      "Epoch 56:\t Train Loss: 2.248 \t Train Acc: 0.161\t Test Acc: 0.099\n",
      "Epoch 58:\t Train Loss: 2.245 \t Train Acc: 0.165\t Test Acc: 0.097\n",
      "Epoch 60:\t Train Loss: 2.243 \t Train Acc: 0.165\t Test Acc: 0.097\n",
      "Epoch 62:\t Train Loss: 2.241 \t Train Acc: 0.167\t Test Acc: 0.096\n",
      "Epoch 64:\t Train Loss: 2.238 \t Train Acc: 0.167\t Test Acc: 0.098\n",
      "Epoch 66:\t Train Loss: 2.237 \t Train Acc: 0.168\t Test Acc: 0.098\n",
      "Epoch 68:\t Train Loss: 2.234 \t Train Acc: 0.170\t Test Acc: 0.096\n",
      "Epoch 70:\t Train Loss: 2.232 \t Train Acc: 0.172\t Test Acc: 0.098\n",
      "Epoch 72:\t Train Loss: 2.230 \t Train Acc: 0.174\t Test Acc: 0.098\n",
      "Epoch 74:\t Train Loss: 2.228 \t Train Acc: 0.176\t Test Acc: 0.096\n",
      "Epoch 76:\t Train Loss: 2.226 \t Train Acc: 0.174\t Test Acc: 0.097\n",
      "Epoch 78:\t Train Loss: 2.224 \t Train Acc: 0.177\t Test Acc: 0.095\n",
      "Epoch 80:\t Train Loss: 2.223 \t Train Acc: 0.180\t Test Acc: 0.100\n",
      "Epoch 82:\t Train Loss: 2.219 \t Train Acc: 0.180\t Test Acc: 0.096\n",
      "Epoch 84:\t Train Loss: 2.219 \t Train Acc: 0.180\t Test Acc: 0.097\n",
      "Epoch 86:\t Train Loss: 2.217 \t Train Acc: 0.183\t Test Acc: 0.094\n",
      "Epoch 88:\t Train Loss: 2.216 \t Train Acc: 0.183\t Test Acc: 0.100\n",
      "Epoch 90:\t Train Loss: 2.214 \t Train Acc: 0.183\t Test Acc: 0.099\n",
      "Epoch 92:\t Train Loss: 2.212 \t Train Acc: 0.185\t Test Acc: 0.096\n",
      "Epoch 94:\t Train Loss: 2.211 \t Train Acc: 0.185\t Test Acc: 0.100\n",
      "Epoch 96:\t Train Loss: 2.211 \t Train Acc: 0.187\t Test Acc: 0.096\n",
      "Epoch 98:\t Train Loss: 2.207 \t Train Acc: 0.188\t Test Acc: 0.098\n",
      "Epoch 100:\t Train Loss: 2.207 \t Train Acc: 0.187\t Test Acc: 0.100\n",
      "Epoch 102:\t Train Loss: 2.205 \t Train Acc: 0.190\t Test Acc: 0.094\n",
      "Epoch 104:\t Train Loss: 2.203 \t Train Acc: 0.190\t Test Acc: 0.101\n",
      "Epoch 106:\t Train Loss: 2.202 \t Train Acc: 0.190\t Test Acc: 0.096\n",
      "Epoch 108:\t Train Loss: 2.201 \t Train Acc: 0.192\t Test Acc: 0.099\n",
      "Epoch 110:\t Train Loss: 2.202 \t Train Acc: 0.190\t Test Acc: 0.099\n",
      "Epoch 112:\t Train Loss: 2.200 \t Train Acc: 0.194\t Test Acc: 0.097\n",
      "Epoch 114:\t Train Loss: 2.198 \t Train Acc: 0.192\t Test Acc: 0.099\n",
      "Epoch 116:\t Train Loss: 2.195 \t Train Acc: 0.193\t Test Acc: 0.100\n",
      "Epoch 118:\t Train Loss: 2.197 \t Train Acc: 0.194\t Test Acc: 0.099\n",
      "Epoch 120:\t Train Loss: 2.195 \t Train Acc: 0.195\t Test Acc: 0.101\n",
      "Epoch 122:\t Train Loss: 2.194 \t Train Acc: 0.193\t Test Acc: 0.098\n",
      "Epoch 124:\t Train Loss: 2.194 \t Train Acc: 0.195\t Test Acc: 0.100\n",
      "Epoch 126:\t Train Loss: 2.192 \t Train Acc: 0.197\t Test Acc: 0.100\n",
      "Epoch 128:\t Train Loss: 2.192 \t Train Acc: 0.196\t Test Acc: 0.098\n",
      "Epoch 130:\t Train Loss: 2.190 \t Train Acc: 0.196\t Test Acc: 0.100\n",
      "Epoch 132:\t Train Loss: 2.189 \t Train Acc: 0.200\t Test Acc: 0.102\n",
      "Epoch 134:\t Train Loss: 2.189 \t Train Acc: 0.198\t Test Acc: 0.100\n",
      "Epoch 136:\t Train Loss: 2.188 \t Train Acc: 0.198\t Test Acc: 0.100\n",
      "Epoch 138:\t Train Loss: 2.187 \t Train Acc: 0.199\t Test Acc: 0.102\n",
      "Epoch 140:\t Train Loss: 2.188 \t Train Acc: 0.199\t Test Acc: 0.101\n",
      "Epoch 142:\t Train Loss: 2.185 \t Train Acc: 0.202\t Test Acc: 0.102\n",
      "Epoch 144:\t Train Loss: 2.186 \t Train Acc: 0.199\t Test Acc: 0.103\n",
      "Epoch 146:\t Train Loss: 2.184 \t Train Acc: 0.201\t Test Acc: 0.099\n",
      "Epoch 148:\t Train Loss: 2.184 \t Train Acc: 0.201\t Test Acc: 0.102\n",
      "Epoch 150:\t Train Loss: 2.182 \t Train Acc: 0.199\t Test Acc: 0.103\n",
      "Epoch 152:\t Train Loss: 2.181 \t Train Acc: 0.203\t Test Acc: 0.098\n",
      "Epoch 154:\t Train Loss: 2.181 \t Train Acc: 0.203\t Test Acc: 0.102\n",
      "Epoch 156:\t Train Loss: 2.179 \t Train Acc: 0.203\t Test Acc: 0.104\n",
      "Epoch 158:\t Train Loss: 2.179 \t Train Acc: 0.204\t Test Acc: 0.097\n",
      "Epoch 160:\t Train Loss: 2.181 \t Train Acc: 0.203\t Test Acc: 0.098\n",
      "Epoch 162:\t Train Loss: 2.177 \t Train Acc: 0.204\t Test Acc: 0.097\n",
      "Epoch 164:\t Train Loss: 2.178 \t Train Acc: 0.205\t Test Acc: 0.099\n",
      "Epoch 166:\t Train Loss: 2.176 \t Train Acc: 0.206\t Test Acc: 0.102\n",
      "Epoch 168:\t Train Loss: 2.175 \t Train Acc: 0.209\t Test Acc: 0.100\n",
      "Epoch 170:\t Train Loss: 2.176 \t Train Acc: 0.206\t Test Acc: 0.099\n",
      "Epoch 172:\t Train Loss: 2.175 \t Train Acc: 0.205\t Test Acc: 0.100\n",
      "Epoch 174:\t Train Loss: 2.174 \t Train Acc: 0.206\t Test Acc: 0.097\n",
      "Epoch 176:\t Train Loss: 2.173 \t Train Acc: 0.208\t Test Acc: 0.100\n",
      "Epoch 178:\t Train Loss: 2.173 \t Train Acc: 0.206\t Test Acc: 0.094\n",
      "Epoch 180:\t Train Loss: 2.172 \t Train Acc: 0.209\t Test Acc: 0.098\n",
      "Epoch 182:\t Train Loss: 2.172 \t Train Acc: 0.209\t Test Acc: 0.098\n",
      "Epoch 184:\t Train Loss: 2.171 \t Train Acc: 0.209\t Test Acc: 0.098\n",
      "Epoch 186:\t Train Loss: 2.169 \t Train Acc: 0.209\t Test Acc: 0.093\n",
      "Epoch 188:\t Train Loss: 2.169 \t Train Acc: 0.210\t Test Acc: 0.097\n",
      "Epoch 190:\t Train Loss: 2.170 \t Train Acc: 0.211\t Test Acc: 0.098\n",
      "Epoch 192:\t Train Loss: 2.169 \t Train Acc: 0.208\t Test Acc: 0.099\n",
      "Epoch 194:\t Train Loss: 2.168 \t Train Acc: 0.209\t Test Acc: 0.099\n",
      "Epoch 196:\t Train Loss: 2.168 \t Train Acc: 0.209\t Test Acc: 0.096\n",
      "Epoch 198:\t Train Loss: 2.168 \t Train Acc: 0.210\t Test Acc: 0.099\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "training_errors = []\n",
    "testing_errors = []\n",
    "training_acc = []\n",
    "testing_acc = []\n",
    "for tr_label, te_label in zip(training_labels, testing_labels):\n",
    "    model = make_cnn(c=4)\n",
    "    epochs = 200\n",
    "    temp_train_dl = make_loader(TensorDataset(X_tr, tr_label))\n",
    "    temp_test_dl = make_loader(TensorDataset(X_te, te_label))\n",
    "    start_lr = 0.05\n",
    "    for i in range(epochs):\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=start_lr - (i/epochs) * start_lr * 0.7)\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, temp_train_dl, opt, k=400, printer=False)\n",
    "        test_loss, test_acc = evaluate(model, temp_test_dl)\n",
    "        \n",
    "        if i % (epochs//100) == 0: \n",
    "            print(f'Epoch {i}:\\t Train Loss: {train_loss:.3f} \\t Train Acc: {train_acc:.3f}\\t ' + \n",
    "                  f'Test Acc: {test_acc:.3f}')\n",
    "    training_errors.append(train_loss)\n",
    "    training_acc.append(train_acc)\n",
    "    testing_errors.append(test_loss)\n",
    "    testing_acc.append(test_acc)\n",
    "\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2929b0f450bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_lr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstart_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_train_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_test_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1d43842ff6ff>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_dl, opt, k, printer)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mxB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[1;32m    140\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                             self.return_indices)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     return torch.max_pool2d(\n\u001b[0;32m--> 487\u001b[0;31m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m max_pool2d = boolean_dispatch(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models2 = []\n",
    "training_errors2 = []\n",
    "testing_errors2 = []\n",
    "training_acc2 = []\n",
    "testing_acc2 = []\n",
    "for tr_label, te_label in zip(training_labels, testing_labels):\n",
    "    model = make_cnn(c=64)\n",
    "    epochs = 200\n",
    "    temp_train_dl = make_loader(TensorDataset(X_tr, tr_label))\n",
    "    temp_test_dl = make_loader(TensorDataset(X_te, te_label))\n",
    "    start_lr = 0.05\n",
    "    for i in range(epochs):\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=start_lr - (i/epochs) * start_lr * 0.7)\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, temp_train_dl, opt, k=400, printer=False)\n",
    "        test_loss, test_acc = evaluate(model, temp_test_dl)\n",
    "        \n",
    "        if i % (epochs//100) == 0: \n",
    "            print(f'Epoch {i}:\\t Train Loss: {train_loss:.3f} \\t Train Acc: {train_acc:.3f}\\t ' + \n",
    "                  f'Test Acc: {test_acc:.3f}')\n",
    "        if train_loss < 0.002:\n",
    "            print('Stopping model after reaching 0 training loss')\n",
    "            break\n",
    "    training_errors2.append(train_loss)\n",
    "    training_acc2.append(train_acc)\n",
    "    testing_errors2.append(test_loss)\n",
    "    testing_acc2.append(test_acc)\n",
    "\n",
    "    models2.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8387285189056396, 1.9739186634063721, 2.294217502822876]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
